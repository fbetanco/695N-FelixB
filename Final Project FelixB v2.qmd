---
title: "Final Project FB - working doc"
subtitle: "DACSS 695N Social Network Analysis"
author: "E. Song/ Felix Betancourt"
date: "April 21, 2024"
format: 
  html:
    toc: true
    toc-depth: 2
    toc-title: Contents
    toc-location: left
    code-fold: false
    html-math-method: katex
    theme: flatly
    smooth-scroll: true
    link-external-icon: true
    link-external-newwindow: true
    citations-hover: true
    footnotes-hover: true
    font-size: 80%
editor: visual
---

## Final Project - Network on subreddit r/politics

### Introduction

I am interested in understanding how social media users influence each other and create communities around specific topics.

Specifically, I would like to explore this topic using Reddit to understand more in the context of domestic Politics.

### Research Question

In particular:

1.  How are Reddit users connected/related in the "Politics" subreddit (r/politics), particularly when it comes to topics related to Biden and Trump?

2.  Are there different communities (networks) for Biden and Trump?

3.  Is there a relationship between "upvotes" for a post, number of comments and how it is related to the key users in the network?

```{r}
library(tidytext)
library(dplyr)
library(tidyverse)
library(quanteda)
library(quanteda.textplots)
#library(RedditExtractoR)
library(RCurl)
```

### Data

1.  I scraped data from the Politics subreddit (r/politics) on April 2nd 2024 using R (RedditExtractoR package)

The code below is inactive because it can take a lot of time to run it. So I already scrapped the data and saved the files as csv.

```{r}
#politics_reddit <- find_thread_urls(subreddit = "politics", sort_by="new", period = "day")

#politics_reddit_comments <- get_thread_content(politics_reddit$url)

#Separate the lists into different objects

#politics_list1 <- politics_reddit_comments[[1]] 
#politics_list2 <- politics_reddit_comments[[2]]

### Saving all the lists in csv to avoid the need to scrap the data several times

#write.csv(politics_reddit, file = "C:/Users/fbeta/OneDrive/1_UMASS_DACSS/695N - Social Network/R-695N-Network/Final Project/data/politics_comments1.csv")

#write.csv(politics_list1, file = "C:/Users/fbeta/OneDrive/1_UMASS_DACSS/695N - Social Network/R-695N-Network/Final Project/data/politics_comments2.csv")

#write.csv(politics_list2, file = "C:/Users/fbeta/OneDrive/1_UMASS_DACSS/695N - Social Network/R-695N-Network/Final Project/data/politics_comments3.csv")

```

2.  This subreddit has 8.5 million users, so the data can be very extensive, however the package used here (RedditExtractoR) pulled the last 1000 post only.

```{r}
getwd()
politik1 <- read.csv("politics_comments1.csv")
politik2 <- read.csv("politics_comments2.csv")
politik3 <- read.csv("politics_comments3.csv")
head(politik1)
head(politik2)
head(politik3)

```

As we can see the the information in object "politik1" is redundant with the information in "politik2" so I won't use "politik1" at all. "Politik2" contain information about the title of the post, author, and some numeric information like up-down votes, number of replies to the post. "politik3" contain detailed comments on each post.

I'll use Text as Data methods to identify key words in the title of the posts (like Biden-Trump, or other topics of interest).

Let's do some data wrangling:

```{r}

# Cleaning and wrangling

politik_df <- politik2 %>% select(-X, -timestamp) #eliminating non-relevant columns
politik_df <- as_tibble(politik_df)
politik_df$date <- as.Date(politik_df$date, format = "%m/%d/%Y")


politik_df2 <- politik3 %>% select(-X, -timestamp) #eliminating non-relevant columns
politik_df2 <- as_tibble(politik_df2)
politik_df2$date <- as.Date(politik_df2$date, format = "%m/%d/%Y")


```

Looking at the comments from user "Automoderator", it is like a Reddit moderator bot reminding rules of the forum, so I'll delete the rows belonging to AutoModerator". Also there are few commments "deleted".

```{r}

politik_df2 <- politik_df2[-(which(politik_df2$author %in% "AutoModerator")),]
politik_df3 <- politik_df2[-(which(politik_df2$author %in% "[deleted]")),]

```


Let's explore a bit about the authors (nodes).

```{r}

#let's create some tables to see frequencies and totals

#first I created a count column
politik_df3 <- politik_df3 %>% mutate(countid = "1")
politik_df3$countid <- as.numeric(politik_df3$countid)
head(politik_df3)

#preparing tables
library(data.table)
politik_table2 <- data.table(politik_df3)

#total posts grouped by author
count_table2 <- politik_table2 %>% group_by(author) %>% summarise(Total_posts = sum(countid))
count_table2 <- count_table2 %>% arrange(desc(Total_posts))
print(count_table2)

summary_votes <- politik_table2 %>% group_by(author) %>% summarize(Total_Score = sum(score))
summary_votes <- summary_votes %>% arrange(desc(Total_Score))
print(summary_votes)

#Upvotes as a proportion of comments
summary_votes_ratio <- politik_table2 %>% group_by(author) %>% summarize(Ratio_upvotes_per_comment = sum(upvotes)/sum(countid))
summary_votes_ratio <- summary_votes_ratio %>% arrange(desc(Ratio_upvotes_per_comment))
print(summary_votes_ratio)

#How many authors (nodes) we have here?
unique(politik_df3$author)

```

About +6k users/authors (nodes), this is too much nodes for the purpose of my research, so I'll select a random sample of posts to analyze.

I'll select the top 10% posts with more comments.

```{r}
#first let's see the distribution of number of comments
percentiles <- quantile(politik_df$comments, probs = c(0.25, 0.50, 0.75, 0.90, 0.95, 0.99))
print(percentiles)

```

Let's subset the df with the top 10% posts in terms of comments and let's see how many posts we have.

```{r}

subset_politik2 <- subset(politik_df, comments >= 1439 )
dim(subset_politik2)

unique(subset_politik2$url)

```

We got a df with 10 original posts and 10 authors, this is now a more "reasonable" data frame to analyze.

Now I need to identify these post into the "politik_df3" which contain all the hierarchical comments network.

```{r}

subset_politik3 <- politik_df3 %>%
         filter(url %in% subset_politik2$url)

unique(subset_politik3$url)


```

I'll keep only the relevant columns:

```{r}
# I'll create a df from hierarchical structure related to comments
politik_comments <- select(subset_politik3, c("url", "author", "comment", "comment_id"))

unique(politik_comments$url)

```

Now I'll try to build the matrix object to start the network analysis.

```{r}

# Extract the levels of each comment and its hierarchy
politik_comment2 <- politik_comments %>%
  mutate(Level = str_count(comment_id, pattern = "_") + 1,  # Count underscores to determine depth
         ParentID = ifelse(Level > 1, sapply(strsplit(comment_id, "_"), function(x) paste(x[-length(x)], collapse = "_")), NA))

```

```{r}
# Create a square matrix initialized to 0
n <- nrow(politik_comment2)
matrix_relation <- matrix(0, nrow = n, ncol = n, dimnames = list(politik_comment2$comment_id, politik_comment2$comment_id))

# Populate the matrix with parent-child relationships
for (i in 1:n) {
  parent_id <- politik_comment2$ParentID[i]
  if (!is.na(parent_id) && parent_id %in% politik_comment2$comment_id) {
    matrix_relation[politik_comment2$comment_id == parent_id, politik_comment2$comment_id[i]] <- 1
  }
}

print(matrix_relation)


```
